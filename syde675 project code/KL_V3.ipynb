{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d62886",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3d62886",
    "outputId": "5b6c6add-96fe-497e-b730-a03d581202a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\kaka\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Resize\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "from torch import Tensor\n",
    "#!pip install einops\n",
    "from torchvision.models import resnet18\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4f240a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef4f240a",
    "outputId": "3558134a-330c-48e7-87fb-10bc5a8db7c0"
   },
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# x_train = load('/content/drive/MyDrive/Colab_data/x_train.npy')\n",
    "# y_train = load('/content/drive/MyDrive/Colab_data/y_train.npy')\n",
    "# x_test = load('/content/drive/MyDrive/Colab_data/x_test.npy')\n",
    "# y_test = load('/content/drive/MyDrive/Colab_data/y_test.npy')\n",
    "\n",
    "x_train = load('x_train.npy')\n",
    "y_train = load('y_train.npy')\n",
    "x_test = load('x_test.npy')\n",
    "y_test = load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2681bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape((-1,3, 224, 224))\n",
    "x_test = x_test.reshape((-1,3, 224, 224))\n",
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be96fd6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be96fd6b",
    "outputId": "baa7b049-7d68-431e-ab61-6c1f494c235b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebadbe99",
   "metadata": {
    "id": "ebadbe99"
   },
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset( Tensor(x_train), Tensor(y_train).long() )\n",
    "dataset_test = TensorDataset( Tensor(x_test), Tensor(y_test).long())\n",
    "\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(image_size),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # transforms.Resize(image_size),\n",
    "    # transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transformations to custom datasets\n",
    "dataset_train.transform = train_transform\n",
    "dataset_test.transform = test_transform\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ec5ff0",
   "metadata": {
    "id": "f7ec5ff0"
   },
   "outputs": [],
   "source": [
    "# # Define Vision Transformer model\n",
    "# class ViT(nn.Module):\n",
    "#     def __init__(self, image_size, patch_size, num_classes, dim):\n",
    "#         super(ViT, self).__init__()\n",
    "#         self.patch_embedding = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "#         num_patches = (image_size // patch_size) ** 2\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "#         self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "#         # Adjust the configuration to set batch_first=True\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=8, batch_first=True)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "#         self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 3, 1, 2)  # Transpose dimensions to match expected input shape\n",
    "#         B, C, H, W = x.shape\n",
    "#         x = self.patch_embedding(x)\n",
    "#         x = rearrange(x, 'b c h w -> b (h w) c')  # flatten spatial dimensions\n",
    "#         cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x += self.positional_embedding\n",
    "#         x = self.transformer_encoder(x)\n",
    "#         x = x[:, 0]  # take the cls token\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Training parameters\n",
    "\n",
    "# image_size = 224\n",
    "# patch_size = 16\n",
    "# num_classes = 2\n",
    "# dim = 768\n",
    "# num_epochs = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49fe8e",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c49fe8e",
    "outputId": "d7fd7001-48c6-42d5-8ad6-78eed4a05ab6"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "num_epochs = 150\n",
    "\n",
    "model  = MobileViT(\n",
    "    image_size = (image_size, image_size),\n",
    "    dims = [96, 120, 144],\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "    num_classes = 2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters(), lr=1e-3, weight_decay=2e-2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"train_Accuracy: {100 * train_correct / train_total}%\")\n",
    "    print(f\"test_Accuracy: {100 * test_correct / test_total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e9258",
   "metadata": {
    "id": "0b8e9258"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46adb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
